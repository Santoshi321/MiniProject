Mini Project :

1.Extracted data from two tables contacts and customer as cvs files from Oracle RDS instance.

2.Upload the data to your S3 bucket using Python
	# copying csv file to S3 Method 1 :
	import boto3
	ACCESS_KEY = '****************'
	SECRET_KEY = '****************************'
	s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)
	s3.upload_file('/Users/sgandham/desktop/customer.csv','mahiproject','customer.csv')

	#copy files to s3 from python - method 2  
	import boto3
	from botocore.exceptions import NoCredentialsError
	ACCESS_KEY = '***************'
	SECRET_KEY = '******************************'
	def upload_to_aws(local_file, bucket, s3_file):
	    s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY,
	                      aws_secret_access_key=SECRET_KEY)
			try:
	        s3.upload_file(local_file, bucket, s3_file)
	        print("Upload Successful")
	        return True
	    except FileNotFoundError:
	        print("The file was not found")
	        return False
	    except NoCredentialsError:
	        print("Credentials not available")
	        return False
	uploaded = upload_to_aws('/Users/sgandham/desktop/customer.csv', 'mahiproject', 'customer_test.csv')

3.Imported the data into Snowflake from the s3 bucket.

	CREATE or replace TABLE CONTACTS
	(CONTACT_ID NUMBER , 
	FIRST_NAME VARCHAR2(255 BYTE), 
	LAST_NAME VARCHAR2(255 BYTE), 
	EMAIL VARCHAR2(255 BYTE), 
	PHONE VARCHAR2(20 BYTE), 
	CUSTOMER_ID NUMBER ) ;

	CREATE TABLE CUSTOMER 
	   (	CUSTOMER_ID NUMBER  , 
		NAME VARCHAR2(255 BYTE), 
		ADDRESS VARCHAR2(255 BYTE), 
	    WEBSITE VARCHAR2(255 BYTE), 
		CREDIT_LIMIT NUMBER(8,2)
	   );

	copy into Contacts
	 from s3://mahiproject/ credentials=(aws_key_id='**************' aws_secret_key='****************************')
	 file_format = (type = csv field_optionally_enclosed_by='"' skip_header = 1 null_if = ('NULL', 'null') empty_field_as_null = true)
	 pattern = '.*contacts.csv'
	 on_error = 'skip_file';

	copy into Customer
	 from s3://mahiproject/ credentials=(aws_key_id='**************' aws_secret_key='****************************')
	 file_format = (type = csv field_optionally_enclosed_by='"' skip_header = 1 null_if = ('NULL', 'null') empty_field_as_null = true)
	 pattern = '.*customer.csv'
	 on_error = 'skip_file';

 4.Exceuted the below query in snowsql from the customer table.

	 CREATE VIEW tempview AS
	    ( SELECT
	        *
	    FROM
	        (
	            WITH qoutput AS (
	                SELECT
	                    c.contact_id,
	                    c.first_name
	                    || ' '
	                    || c.last_name full_name,
	                    ct.address,
	                    ct.credit_limit
	                FROM
	                    ot.contacts    c
	                    JOIN ot.customers   ct ON c.customer_id = ct.customer_id
	                WHERE
	                    ct.credit_limit IN (
	                        100,
	                        500,
	                        1500,
	                        2400
	                    )
	                ORDER BY
	                    ct.credit_limit
	            )
	            SELECT
	                credit_limit,
	                COUNT(*)
	            FROM
	                qoutput
	            GROUP BY
	                credit_limit
	        )
	    );

	select * from tempview;

5.Output of the above query is exported into S3 from Snowflake

	copy into s3://mahiproject/QueryResult
	from Tempview
	file_format=(type=csv compression=None) header=True
	credentials=(aws_key_id='*******************' aws_secret_key='*******************************')
	overwrite=true;

6.Create a basic data Visualization ( graphs ) using the data available on s3 using databricks.

	ACCESS_KEY = "**************"
	SECRET_KEY = "***************************"
	ENCODED_SECRET_KEY = SECRET_KEY.replace("/", "%2F")
	AWS_BUCKET_NAME = "mahiproject/QueryResult_0_0_0.csv"
	MOUNT_NAME = "queryoutputgraph"
	
	dbutils.fs.mount("s3a://%s:%s@%s" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), "/mnt/%s" % MOUNT_NAME)

	display(dbutils.fs.ls("/mnt/queryoutputgraph"))

	newdf = spark.read.format('csv').options(header='true',inferSchema='true').load("dbfs:/mnt/queryoutputgraph/QueryResult_0_0_0.csv")

	newdf.show()

	df1 = newdf.toPandas()

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	newframe = df1
	newframe.plot(x='CREDIT_LIMIT', y='COUNT(*)', kind='bar',rot=50)
	plt.xlabel('CREDIT LIMIT')
	plt.ylabel('NO OF PEOPLE')
	display()

Mini Project :

1.Extracted data from two tables contacts and customer as cvs files from Oracle RDS instance.

2.Upload the data to your S3 bucket using Python
	# copying csv file to S3 Method 1 :
	import boto3
	ACCESS_KEY = '****************'
	SECRET_KEY = '****************************'
	s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)
	s3.upload_file('/Users/sgandham/desktop/customer.csv','mahiproject','customer.csv')

	#copy files to s3 from python - method 2  
	import boto3
	from botocore.exceptions import NoCredentialsError
	ACCESS_KEY = '***************'
	SECRET_KEY = '******************************'
	def upload_to_aws(local_file, bucket, s3_file):
	    s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY,
	                      aws_secret_access_key=SECRET_KEY)
			try:
	        s3.upload_file(local_file, bucket, s3_file)
	        print("Upload Successful")
	        return True
	    except FileNotFoundError:
	        print("The file was not found")
	        return False
	    except NoCredentialsError:
	        print("Credentials not available")
	        return False
	uploaded = upload_to_aws('/Users/sgandham/desktop/customer.csv', 'mahiproject', 'customer_test.csv')

3.Imported the data into Snowflake from the s3 bucket.

	CREATE or replace TABLE CONTACTS
	(CONTACT_ID NUMBER , 
	FIRST_NAME VARCHAR2(255 BYTE), 
	LAST_NAME VARCHAR2(255 BYTE), 
	EMAIL VARCHAR2(255 BYTE), 
	PHONE VARCHAR2(20 BYTE), 
	CUSTOMER_ID NUMBER ) ;

	CREATE TABLE CUSTOMER 
	   (	CUSTOMER_ID NUMBER  , 
		NAME VARCHAR2(255 BYTE), 
		ADDRESS VARCHAR2(255 BYTE), 
	    WEBSITE VARCHAR2(255 BYTE), 
		CREDIT_LIMIT NUMBER(8,2)
	   );

	copy into Contacts
	 from s3://mahiproject/ credentials=(aws_key_id='**************' aws_secret_key='****************************')
	 file_format = (type = csv field_optionally_enclosed_by='"' skip_header = 1 null_if = ('NULL', 'null') empty_field_as_null = true)
	 pattern = '.*contacts.csv'
	 on_error = 'skip_file';

	copy into Customer
	 from s3://mahiproject/ credentials=(aws_key_id='**************' aws_secret_key='****************************')
	 file_format = (type = csv field_optionally_enclosed_by='"' skip_header = 1 null_if = ('NULL', 'null') empty_field_as_null = true)
	 pattern = '.*customer.csv'
	 on_error = 'skip_file';

 4.Exceuted the below query in snowsql from the customer table.

	 CREATE VIEW tempview AS
	    ( SELECT
	        *
	    FROM
	        (
	            WITH qoutput AS (
	                SELECT
	                    c.contact_id,
	                    c.first_name
	                    || ' '
	                    || c.last_name full_name,
	                    ct.address,
	                    ct.credit_limit
	                FROM
	                    ot.contacts    c
	                    JOIN ot.customers   ct ON c.customer_id = ct.customer_id
	                WHERE
	                    ct.credit_limit IN (
	                        100,
	                        500,
	                        1500,
	                        2400
	                    )
	                ORDER BY
	                    ct.credit_limit
	            )
	            SELECT
	                credit_limit,
	                COUNT(*)
	            FROM
	                qoutput
	            GROUP BY
	                credit_limit
	        )
	    );

	select * from tempview;

5.Output of the above query is exported into S3 from Snowflake

	copy into s3://mahiproject/QueryResult
	from Tempview
	file_format=(type=csv compression=None) header=True
	credentials=(aws_key_id='*******************' aws_secret_key='*******************************')
	overwrite=true;

6.Create a basic data Visualization ( graphs ) using the data available on s3 using databricks.

	ACCESS_KEY = "**************"
	SECRET_KEY = "***************************"
	ENCODED_SECRET_KEY = SECRET_KEY.replace("/", "%2F")
	AWS_BUCKET_NAME = "mahiproject/QueryResult_0_0_0.csv"
	MOUNT_NAME = "queryoutputgraph"
	
	dbutils.fs.mount("s3a://%s:%s@%s" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), "/mnt/%s" % MOUNT_NAME)

	display(dbutils.fs.ls("/mnt/queryoutputgraph"))

	newdf = spark.read.format('csv').options(header='true',inferSchema='true').load("dbfs:/mnt/queryoutputgraph/QueryResult_0_0_0.csv")

	newdf.show()

	df1 = newdf.toPandas()

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	newframe = df1
	newframe.plot(x='CREDIT_LIMIT', y='COUNT(*)', kind='bar',rot=50)
	plt.xlabel('CREDIT LIMIT')
	plt.ylabel('NO OF PEOPLE')
	display()

Databricks file :

https://community.cloud.databricks.com/?o=5543523677197837#notebook/2665521677760048/command/2665521677760049




    
